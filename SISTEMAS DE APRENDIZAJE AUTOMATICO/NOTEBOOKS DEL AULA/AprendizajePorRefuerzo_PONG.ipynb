{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APRENDIZAJE POR REFUERZO - PONG USANDO MATPLOTLIB\n",
    "\n",
    "Se intenta realizar una simulación del ambiente del juego y su compotamiento en un Jupyter Notebook.\n",
    "\n",
    "El agente será el “player 1” y sus acciones posible son 2:\n",
    "\n",
    "- mover hacia arriba\n",
    "- mover hacia abajo\n",
    "\n",
    "Las reglas del juego son las siguientes:\n",
    "\n",
    "- El agente tiene 3 vidas.\n",
    "- Si pierde… castigo, restamos 10 puntos.\n",
    "- Cada vez que le demos a la bola, recompensa, sumamos 10.   \n",
    "\n",
    "\n",
    "NOTA: Para que no quede jugando por siempre, limitaremos el juego a 3000 iteraciones máximo o alcanzar 500 puntos (condición ganadora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS NECESARIOS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from math import ceil,floor\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Agente\n",
    "\n",
    "Dentro de la clase Agente (PongAgent) está la tabla donde se van almacenando las políticas. La tabla cuenta de 3 coordenadas:\n",
    "\n",
    "- La posición actual del jugador.\n",
    "- La posición “y” de la pelota.\n",
    "- La posición en el eje “x” de la pelota.\n",
    "\n",
    "Además, en esta clase se define también el factor de descuento, el learning rate y el ratio de exploración.\n",
    "\n",
    "Los métodos más importantes son:\n",
    "\n",
    "- **get_next_step()**: decide la siguiente acción a tomar en base al ratio de exploración si tomar “el mejor paso” que esté almacenado o tomar un paso al azar, dando posibilidad a explorar el ambiente.\n",
    "- **update()**: aquí se actualizan las políticas mediante la ecuación de Bellman que vimos anteriormente. Es su implementación en python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    \n",
    "    def __init__(self, game, policy=None, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion = 0.9):\n",
    "\n",
    "        # Creamos la tabla de politicas\n",
    "        if policy is not None:\n",
    "            self._q_table = policy\n",
    "        else:\n",
    "            position = list(game.positions_space.shape)\n",
    "            position.append(len(game.action_space))\n",
    "            self._q_table = np.zeros(position)\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ratio_explotacion = ratio_explotacion\n",
    "\n",
    "    def get_next_step(self, state, game):\n",
    "        \n",
    "        # Damos un paso aleatorio...\n",
    "        next_step = np.random.choice(list(game.action_space))\n",
    "        \n",
    "        # o tomaremos el mejor paso...\n",
    "        if np.random.uniform() <= self.ratio_explotacion:\n",
    "            # tomar el maximo\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                    self._q_table[state[0],state[1],state[2]] == self._q_table[state[0],state[1],state[2]].max()\n",
    "                ))\n",
    "            next_step = list(game.action_space)[idx_action]\n",
    "\n",
    "        return next_step\n",
    "\n",
    "    # actualizamos las politicas con las recompensas obtenidas\n",
    "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
    "        idx_action_taken =list(game.action_space).index(action_taken)\n",
    "\n",
    "        actual_q_value_options = self._q_table[old_state[0], old_state[1], old_state[2]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\n",
    "        future_max_q_value = reward_action_taken  +  self.discount_factor*future_q_value_options.max()\n",
    "        if reached_end:\n",
    "            future_max_q_value = reward_action_taken #maximum reward\n",
    "\n",
    "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + \\\n",
    "                                              self.learning_rate*(future_max_q_value -actual_q_value)\n",
    "    \n",
    "    def print_policy(self):\n",
    "        for row in np.round(self._q_table,1):\n",
    "            for column in row:\n",
    "                print('[', end='')\n",
    "                for value in column:\n",
    "                    print(str(value).zfill(5), end=' ')\n",
    "                print('] ', end='')\n",
    "            print('')\n",
    "            \n",
    "    def get_policy(self):\n",
    "        return self._q_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Ambiente/Entorno (Environment) \n",
    "En la clase Ambiente (PongEnviroment) se halla implementada la lógica y control del juego del pong. Se controla que la pelota rebote, que no se salga de la pantalla y se encuentran también los métodos para graficar y animar en matplotlib.\n",
    "\n",
    "Por defecto, se define una pantalla de 40 pixeles x 50px de alto y si se utiliza la variable “movimiento_px = 5” quedará definida la tabla de políticas en 8 de alto y 10 de ancho (por hacer 40/5=8 y 50/5=10). Estos valores se pueden modificar a conveniencia\n",
    "\n",
    "Además, se tiene el control de cuándo dar las recompensas y penalizaciones, al perder cada vida y detectar si el juego a terminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongEnvironment:\n",
    "    \n",
    "    def __init__(self, max_life=3, height_px = 40, width_px = 50, movimiento_px = 3):\n",
    "        \n",
    "        self.action_space = ['Arriba','Abajo']\n",
    "        \n",
    "        self._step_penalization = 0\n",
    "        \n",
    "        self.state = [0,0,0]\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.dx = movimiento_px\n",
    "        self.dy = movimiento_px\n",
    "        \n",
    "        filas = ceil(height_px/movimiento_px)\n",
    "        columnas = ceil(width_px/movimiento_px)\n",
    "        \n",
    "        self.positions_space = np.array([[[0 for z in range(columnas)] \n",
    "                                                  for y in range(filas)] \n",
    "                                                     for x in range(filas)])\n",
    "\n",
    "        self.lives = max_life\n",
    "        self.max_life=max_life\n",
    "        \n",
    "        self.x = randint(int(width_px/2), width_px) \n",
    "        self.y = randint(0, height_px-10)\n",
    "        \n",
    "        self.player_alto = int(height_px/4)\n",
    "\n",
    "        self.player1 = self.player_alto  # posic. inicial del player\n",
    "        \n",
    "        self.score = 0\n",
    "        \n",
    "        self.width_px = width_px\n",
    "        self.height_px = height_px\n",
    "        self.radio = 2.5\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.state = [0,0,0]\n",
    "        self.lives = self.max_life\n",
    "        self.score = 0\n",
    "        self.x = randint(int(self.width_px/2), self.width_px) \n",
    "        self.y = randint(0, self.height_px-10)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, animate=False):\n",
    "        self._apply_action(action, animate)\n",
    "        done = self.lives <=0 # final\n",
    "        reward = self.score\n",
    "        reward += self._step_penalization\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward , done\n",
    "\n",
    "    def _apply_action(self, action, animate=False):\n",
    "        \n",
    "        if action == \"Arriba\":\n",
    "            self.player1 += abs(self.dy)\n",
    "        elif action == \"Abajo\":\n",
    "            self.player1 -= abs(self.dy)\n",
    "            \n",
    "        self.avanza_player()\n",
    "\n",
    "        self.avanza_frame()\n",
    "\n",
    "        if animate:\n",
    "            clear_output(wait=True);\n",
    "            fig = self.dibujar_frame()\n",
    "            plt.show()\n",
    "\n",
    "        self.state = (floor(self.player1/abs(self.dy))-2, floor(self.y/abs(self.dy))-2, floor(self.x/abs(self.dx))-2)\n",
    "    \n",
    "    def detectaColision(self, ball_y, player_y):\n",
    "        if (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def avanza_player(self):\n",
    "        if self.player1 + self.player_alto >= self.height_px:\n",
    "            self.player1 = self.height_px - self.player_alto\n",
    "        elif self.player1 <= -abs(self.dy):\n",
    "            self.player1 = -abs(self.dy)\n",
    "\n",
    "    def avanza_frame(self):\n",
    "        self.x += self.dx\n",
    "        self.y += self.dy\n",
    "        if self.x <= 3 or self.x > self.width_px:\n",
    "            self.dx = -self.dx\n",
    "            if self.x <= 3:\n",
    "                ret = self.detectaColision(self.y, self.player1)\n",
    "\n",
    "                if ret:\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.score = -10\n",
    "                    self.lives -= 1\n",
    "                    if self.lives>0:\n",
    "                        self.x = randint(int(self.width_px/2), self.width_px)\n",
    "                        self.y = randint(0, self.height_px-10)\n",
    "                        self.dx = abs(self.dx)\n",
    "                        self.dy = abs(self.dy)\n",
    "        else:\n",
    "            self.score = 0\n",
    "\n",
    "        if self.y < 0 or self.y > self.height_px:\n",
    "            self.dy = -self.dy\n",
    "\n",
    "    def dibujar_frame(self):\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        a1 = plt.gca()\n",
    "        circle = plt.Circle((self.x, self.y), self.radio, fc='slategray', ec=\"black\")\n",
    "        a1.set_ylim(-5, self.height_px+5)\n",
    "        a1.set_xlim(-5, self.width_px+5)\n",
    "\n",
    "        rectangle = plt.Rectangle((-5, self.player1), 5, self.player_alto, fc='gold', ec=\"none\")\n",
    "        a1.add_patch(circle);\n",
    "        a1.add_patch(rectangle)\n",
    "        #a1.set_yticklabels([]);a1.set_xticklabels([]);\n",
    "        plt.text(4, self.height_px, \"PUNTOS:\"+str(self.total_reward)+\"  VIDAS:\"+str(self.lives), fontsize=12)\n",
    "        if self.lives <=0:\n",
    "            plt.text(10, self.height_px-14, \"GAME OVER\", fontsize=16)\n",
    "        elif self.total_reward >= 1000:\n",
    "            plt.text(10, self.height_px-14, \"¡GANASTE!\", fontsize=16)\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El juego: Simular miles de veces para enseñar/entrenar\n",
    "\n",
    "Por último, se define una función para jugar (*play*), donde se indica la cantidad de veces que se quiere iterar la simulación del juego e ir almacenando algunas estadísticas sobre el comportamiento del agente, si mejora el puntaje con las iteraciones y el máximo puntaje alcanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1,\n",
    "         ratio_explotacion=0.9,learner=None, game=None, animate=False):\n",
    "\n",
    "    if game is None:\n",
    "        # si usamos movimiento_px = 5 creamos una tabla de politicas de 8x10\n",
    "        # si usamos movimiento_px = 3 la tabla sera de 14x17\n",
    "        game = PongEnvironment(max_life=max_life, movimiento_px = 3)\n",
    "        \n",
    "    if learner is None:\n",
    "        print(\"¡Arrancando un nuevo Entrenamiento!\")\n",
    "        learner = PongAgent(game, discount_factor = discount_factor,learning_rate = learning_rate, ratio_explotacion= ratio_explotacion)\n",
    "\n",
    "    max_points= -9999\n",
    "    first_max_reached = 0\n",
    "    total_rw=0\n",
    "    steps=[]\n",
    "\n",
    "    for played_games in range(0, rounds):\n",
    "        state = game.reset()\n",
    "        reward, done = None, None\n",
    "        \n",
    "        itera=0\n",
    "        while (done != True) and (itera < 3000 and game.total_reward<=1000):  # iteraciones: 3000 o puntos: 1000\n",
    "            old_state = np.array(state)\n",
    "            next_action = learner.get_next_step(state, game)\n",
    "            state, reward, done = game.step(next_action, animate=animate)\n",
    "            if rounds > 1:\n",
    "                learner.update(game, old_state, next_action, reward, state, done)\n",
    "            itera+=1\n",
    "        \n",
    "        steps.append(itera)\n",
    "        \n",
    "        total_rw+=game.total_reward\n",
    "        if game.total_reward > max_points:\n",
    "            max_points=game.total_reward\n",
    "            first_max_reached = played_games\n",
    "        \n",
    "        if played_games %500==0 and played_games >1 and not animate:\n",
    "            print(\"-- Partidas[\", played_games, \"] Avg.Puntos[\", int(total_rw/played_games),\"]  AVG Steps[\", int(np.array(steps).mean()), \"] Max Score[\", max_points,\"]\")\n",
    "                \n",
    "    if played_games>1:\n",
    "        print('Partidas[',played_games,'] Avg.Puntos[',int(total_rw/played_games),'] Max score[', max_points,'] en partida[',first_max_reached,']')\n",
    "        \n",
    "    #learner.print_policy()  # impresión tabla de políticas\n",
    "    \n",
    "    return learner, game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Para entrenar se ejecuta la función con los siguientes parámetros:\n",
    "\n",
    "- 9000 partidas que se deben jugar\n",
    "- *ratio de explotación*: el 85% de las veces será avaro, pero el 15% elige acciones aleatorias, dando lugar a la exploración.\n",
    "- *learning rate* = se suele dejar en el 10 por ciento como un valor razonable, dando lugar a las recompensas y permitiendo actualizar la importancia de cada acción poco a poco. Tras más iteraciones, mayor importancia tendrá esa acción.\n",
    "- *discount_factor* = También se suele empezar con valor de 0.1 pero aquí se usará un valor del 0.2 para intentar indicar al algoritmo que interesan las recompensas a más largo plazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Arrancando un nuevo Entrenamiento!\n",
      "-- Partidas[ 500 ] Avg.Puntos[ 20 ]  AVG Steps[ 238 ] Max Score[ 160 ]\n",
      "-- Partidas[ 1000 ] Avg.Puntos[ 26 ]  AVG Steps[ 259 ] Max Score[ 160 ]\n",
      "-- Partidas[ 1500 ] Avg.Puntos[ 28 ]  AVG Steps[ 265 ] Max Score[ 200 ]\n",
      "-- Partidas[ 2000 ] Avg.Puntos[ 28 ]  AVG Steps[ 267 ] Max Score[ 280 ]\n",
      "-- Partidas[ 2500 ] Avg.Puntos[ 29 ]  AVG Steps[ 268 ] Max Score[ 280 ]\n",
      "-- Partidas[ 3000 ] Avg.Puntos[ 30 ]  AVG Steps[ 272 ] Max Score[ 280 ]\n",
      "-- Partidas[ 3500 ] Avg.Puntos[ 32 ]  AVG Steps[ 278 ] Max Score[ 410 ]\n",
      "-- Partidas[ 4000 ] Avg.Puntos[ 34 ]  AVG Steps[ 287 ] Max Score[ 410 ]\n",
      "-- Partidas[ 4500 ] Avg.Puntos[ 37 ]  AVG Steps[ 295 ] Max Score[ 490 ]\n",
      "-- Partidas[ 5000 ] Avg.Puntos[ 38 ]  AVG Steps[ 300 ] Max Score[ 490 ]\n",
      "-- Partidas[ 5500 ] Avg.Puntos[ 39 ]  AVG Steps[ 303 ] Max Score[ 630 ]\n",
      "-- Partidas[ 6000 ] Avg.Puntos[ 40 ]  AVG Steps[ 307 ] Max Score[ 630 ]\n",
      "-- Partidas[ 6500 ] Avg.Puntos[ 41 ]  AVG Steps[ 312 ] Max Score[ 630 ]\n",
      "-- Partidas[ 7000 ] Avg.Puntos[ 42 ]  AVG Steps[ 312 ] Max Score[ 630 ]\n",
      "-- Partidas[ 7500 ] Avg.Puntos[ 42 ]  AVG Steps[ 313 ] Max Score[ 630 ]\n",
      "-- Partidas[ 8000 ] Avg.Puntos[ 42 ]  AVG Steps[ 313 ] Max Score[ 630 ]\n",
      "-- Partidas[ 8500 ] Avg.Puntos[ 42 ]  AVG Steps[ 313 ] Max Score[ 630 ]\n",
      "Partidas[ 8999 ] Avg.Puntos[ 42 ] Max score[ 630 ] en partida[ 5106 ]\n"
     ]
    }
   ],
   "source": [
    "learner, game = play(rounds=9000, discount_factor = 0.2, learning_rate = 0.1, ratio_explotacion=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las salidas vemos, sobre todo, cómo va mejorando en la cantidad de “steps” que da el agente antes de perder la partida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando el resultado \n",
    "\n",
    "Ya está el agente entrenado. A continuación, vamos a ver como se comporta en una partida de pong, y lo vamos a ver jugar, pasando el parámetro animate=True.\n",
    "\n",
    "Antes de jugar, se instancia un nuevo agente “learner2” que utilizará las políticas que se han creado anteriormente. A este agente se le *'carga'* el valor de explotación en 1, para evitar que tome pasos aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner2 = PongAgent(game, policy=learner.get_policy())\n",
    "learner2.ratio_explotacion = 1.0  # con esto quitamos las elecciones aleatorias al jugar\n",
    "player = play(rounds=1, learner=learner2, game=game, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ir variando el objetivo a 1000 puntos, la cantidad de vidas, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "En este notebook se ha practicado los conceptos básicos del reinforcement learning y sus características.\n",
    "\n",
    "Se ha realizado una implementación del algoritmo Q-Learning y se ha programado un juego completo en Python en donde el Agente, sin tener conocimiento previo de las reglas ni del entorno, logra aprender y volverse un muy buen jugador de Pong tras miles de simulaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceiadb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
